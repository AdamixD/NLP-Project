{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from training.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFXLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Model(model_path=\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f2dfdbc7-29b0-4954-b27b-cc7c29d614c7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f2dfdbc7-29b0-4954-b27b-cc7c29d614c7/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/991 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbeee0e69e674f4599cae6055f0c8dec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2c66619a-5c83-45c4-9987-62918708bf13/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2c66619a-5c83-45c4-9987-62918708bf13/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/111 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "444dcd354b1d4c1cb0498c2b9c066142"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_json = pd.read_json(\"data/preprocessed/espanol2\", orient=\"records\", lines=True)\n",
    "tf_train, tf_test = model.prepare_train_test_data(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.5943 - f1_m: 0.1136"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[192001536,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:OneHot]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m log_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogs/fit/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspanish_xlm-roberta-base\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      6\u001B[0m tensorboard_callback \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mTensorBoard(log_dir\u001B[38;5;241m=\u001B[39mlog_dir, histogram_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtensorboard_callback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\SEM6\\NLP\\NLP_last_3\\NLP-Fake-News-Detection\\training\\model.py:68\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, train_data, validation_data, callbacks, epochs)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, train_data, validation_data, callbacks, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m---> 68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:194\u001B[0m, in \u001B[0;36mhistogram\u001B[1;34m(name, data, step, buckets, description)\u001B[0m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;129m@lazy_tensor_creator\u001B[39m\u001B[38;5;241m.\u001B[39mLazyTensorCreator\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlazy_tensor\u001B[39m():\n\u001B[0;32m    192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _buckets(data, buckets)\n\u001B[1;32m--> 194\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39msummary\u001B[38;5;241m.\u001B[39mwrite(\n\u001B[0;32m    195\u001B[0m     tag\u001B[38;5;241m=\u001B[39mtag,\n\u001B[0;32m    196\u001B[0m     tensor\u001B[38;5;241m=\u001B[39mlazy_tensor,\n\u001B[0;32m    197\u001B[0m     step\u001B[38;5;241m=\u001B[39mstep,\n\u001B[0;32m    198\u001B[0m     metadata\u001B[38;5;241m=\u001B[39msummary_metadata,\n\u001B[0;32m    199\u001B[0m )\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\util\\lazy_tensor_creator.py:66\u001B[0m, in \u001B[0;36mLazyTensorCreator.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     65\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor \u001B[38;5;241m=\u001B[39m _CALL_IN_PROGRESS_SENTINEL\n\u001B[1;32m---> 66\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor_callable()\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tensor\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:192\u001B[0m, in \u001B[0;36mhistogram.<locals>.lazy_tensor\u001B[1;34m()\u001B[0m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;129m@lazy_tensor_creator\u001B[39m\u001B[38;5;241m.\u001B[39mLazyTensorCreator\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlazy_tensor\u001B[39m():\n\u001B[1;32m--> 192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_buckets\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuckets\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:291\u001B[0m, in \u001B[0;36m_buckets\u001B[1;34m(data, bucket_count)\u001B[0m\n\u001B[0;32m    285\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mtranspose(a\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mstack([edges, edges, bucket_counts]))\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcond(\n\u001B[0;32m    288\u001B[0m         has_single_value, when_single_value, when_multiple_values\n\u001B[0;32m    289\u001B[0m     )\n\u001B[1;32m--> 291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mcond(is_empty, when_empty, when_nonempty)\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:287\u001B[0m, in \u001B[0;36m_buckets.<locals>.when_nonempty\u001B[1;34m()\u001B[0m\n\u001B[0;32m    281\u001B[0m     bucket_counts \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m    282\u001B[0m         tf\u001B[38;5;241m.\u001B[39mconcat([zeroes[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], [data_size]], \u001B[38;5;241m0\u001B[39m)[:bucket_count],\n\u001B[0;32m    283\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    284\u001B[0m     )\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mtranspose(a\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mstack([edges, edges, bucket_counts]))\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcond\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_single_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhen_single_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhen_multiple_values\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\yaroslav\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:256\u001B[0m, in \u001B[0;36m_buckets.<locals>.when_nonempty.<locals>.when_multiple_values\u001B[1;34m()\u001B[0m\n\u001B[0;32m    252\u001B[0m clamped_indices \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mminimum(bucket_indices, bucket_count \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# Use float64 instead of float32 to avoid accumulating floating point error\u001B[39;00m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# later in tf.reduce_sum when summing more than 2^24 individual `1.0` values.\u001B[39;00m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;66;03m# See https://github.com/tensorflow/tensorflow/issues/51419 for details.\u001B[39;00m\n\u001B[1;32m--> 256\u001B[0m one_hots \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mone_hot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclamped_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbucket_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    259\u001B[0m bucket_counts \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m    260\u001B[0m     tf\u001B[38;5;241m.\u001B[39mreduce_sum(input_tensor\u001B[38;5;241m=\u001B[39mone_hots, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[0;32m    261\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    262\u001B[0m )\n\u001B[0;32m    263\u001B[0m edges \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mlinspace(min_, max_, bucket_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: OOM when allocating tensor with shape[192001536,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:OneHot]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "model.compile()\n",
    "log_dir = \"logs/fit/\" + \"spanish_xlm-roberta-base\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_data=tf_train, epochs=5, validation_data=tf_test, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"model/spanish2/spanish_xlm_roberta.h5py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_model(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
